
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>8. Slurm Jobs &#8212; Crunchomics 0.01 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="9. Using Slurm Indirectly" href="slurm_indirect.html" />
    <link rel="prev" title="7. Slurm Overview" href="slurm_overview.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="slurm_indirect.html" title="9. Using Slurm Indirectly"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="slurm_overview.html" title="7. Slurm Overview"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Crunchomics 0.01 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">8. </span>Slurm Jobs</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="slurm-jobs">
<h1><span class="section-number">8. </span>Slurm Jobs<a class="headerlink" href="#slurm-jobs" title="Permalink to this headline">¶</a></h1>
<div class="section" id="jobs-and-job-steps">
<h2><span class="section-number">8.1. </span>Jobs and Job Steps<a class="headerlink" href="#jobs-and-job-steps" title="Permalink to this headline">¶</a></h2>
<ul>
<li><p>A compute job can consist of several steps. For example: you download the files, you down sample and then you do an alignment. These steps are job steps and are invoked by <code class="docutils literal notranslate"><span class="pre">srun</span></code> from with in the batch script. Each srun command in a batch script can ask for its own set of resources as long as it fits in the allocation. In other words the srun commands in a batch script are bounded by the allocation for the batch script. For example an srun can not ask for more cpu’s than asked for in the sbatch file.</p></li>
<li><dl>
<dt>Some useful srun flags are:</dt><dd><ul>
<li><dl class="option-list">
<dt><kbd><span class="option">-c</span>, <span class="option">--cpus-per-task=<var>ncpus</var></span></kbd></dt>
<dd><p>number of cpus required per task</p>
</dd>
</dl>
</li>
<li><dl class="option-list">
<dt><kbd><span class="option">-n</span>, <span class="option">--ntasks=<var>ntasks</var></span></kbd></dt>
<dd><p>number of tasks to run</p>
</dd>
</dl>
</li>
<li><dl class="option-list">
<dt><kbd><span class="option">-N</span>, <span class="option">--nodes=<var>N</var></span></kbd></dt>
<dd><p>number of nodes on which to run (N = min[-max])</p>
</dd>
</dl>
</li>
<li><dl class="option-list">
<dt><kbd><span class="option">-o</span>, <span class="option">--output=<var>out</var></span></kbd></dt>
<dd><p>location of stdout redirection</p>
</dd>
</dl>
</li>
<li><dl class="option-list">
<dt><kbd><span class="option">-w</span>, <span class="option">--nodelist=<var>hosts</var></span></kbd></dt>
<dd><p>request a specific list of hosts</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><p>Print the name of the node with the command <code class="docutils literal notranslate"><span class="pre">hostname</span></code></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun hostname
<span class="c1">#omics-cn004  this job was allocated to cn004</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Carry out this task four times:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun -n4 hostname
<span class="c1">#omics-cn004</span>
<span class="c1">#omics-cn004</span>
<span class="c1">#omics-cn004</span>
<span class="c1">#omics-cn004  again allocation on cn004</span>
</pre></div>
</div>
<p>Note that the effect of the -n4 flag is that the program (hostname) is automatically started 4 times. For software which runs on multiple nodes and commucates between instances through mpi this is useful. In case of a multi threaded program it is usually not what you want.</p>
<ul class="simple">
<li><p>Now, ask for four nodes</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun -N4 hostname
<span class="c1">#omics-cn002</span>
<span class="c1">#omics-cn003</span>
<span class="c1">#omics-cn001</span>
<span class="c1">#omics-cn004  now, cn001, cn002, cn003 and cn004 are allocated</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Ask for a specific host:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun -n2 -w omics-cn002 hostname
<span class="c1">#omics-cn002</span>
<span class="c1">#omics-cn002</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Output to a file (here: hn.txt that is stored in your current directory)</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>srun -N3 -n5 -o hn.txt hostname
cat hn.txt
<span class="c1">#omics-cn001</span>
<span class="c1">#omics-cn001</span>
<span class="c1">#omics-cn002</span>
<span class="c1">#omics-cn002</span>
<span class="c1">#omics-cn003  #001 and 002 are used twice, 003 is used once</span>
</pre></div>
</div>
<ul class="simple">
<li><p>A job consists in two parts: resource requests and job steps. Resource requests consist in a number of CPUs, computing expected duration, amounts of RAM or disk space, etc. Job steps describe tasks that must be done, software which must be run.</p></li>
<li><p>The typical way of creating a job is to write a submission script. A submission script is a shell script, e.g. a Bash script, whose comments, if they are prefixed with SBATCH, are understood by Slurm as parameters describing resource requests and other submissions options. You can get the complete list of parameters from the sbatch manpage <code class="docutils literal notranslate"><span class="pre">man</span> <span class="pre">sbatch</span></code>.</p></li>
<li><p>get hints for writing a job script at the script generator wizzard <a class="reference external" href="http://www.ceci-hpc.be/scriptgen.html">Script Generator Wizzard Ceci</a> Ignore the cluster names and replace <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=defq</span></code> with <code class="docutils literal notranslate"><span class="pre">#SBATCH</span> <span class="pre">--partition=all</span></code>).</p></li>
</ul>
</div>
<div class="section" id="batch-jobs-sbatch">
<h2><span class="section-number">8.2. </span>Batch jobs: sbatch<a class="headerlink" href="#batch-jobs-sbatch" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><dl class="simple">
<dt>Make a text file with the content as in the box below and save it as batch1.sh:</dt><dd><ul>
<li><p>it writes the output to the file res.txt</p></li>
<li><p>it consists of one task</p></li>
<li><p>it allocates 10 minutes of compute time and 10 MB memory</p></li>
<li><p>I expect this script to run at least 15 seconds and to print 3 hostnames and 3 dates.</p></li>
<li><p>I can execute the script with <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">sbatch1.sh</span></code> and monitor the script with <code class="docutils literal notranslate"><span class="pre">squeue</span></code></p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --job-name=batch1</span>
<span class="c1">#SBATCH --output=res.txt</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --time=10:00</span>
<span class="c1">#SBATCH --mem-per-cpu=10</span>
<span class="c1">#</span>
srun hostname
srun sleep <span class="m">5</span>
srun date
srun hostname
srun sleep <span class="m">5</span>
srun date
srun hostname
srun sleep <span class="m">5</span>
srun date
</pre></div>
</div>
<ul class="simple">
<li><p>execute the script and monitor it</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch batch1.sh
squeue
</pre></div>
</div>
<ul>
<li><p>This is the output:</p>
<blockquote>
<div><blockquote>
<div><div class="figure align-center">
<a class="reference internal image-reference" href="_images/slurm3.png"><img alt="_images/slurm3.png" src="_images/slurm3.png" style="width: 500px; height: 160px;" /></a>
</div>
</div></blockquote>
<ul class="simple">
<li><p>The job goes through the PENDING state (PD), then enters the RUNNING state (R) and finally goes to the COMPLETED state, or FAILED state.</p></li>
<li><p>Indeed 3 times hostname and 3 times date some 7 seconds apart</p></li>
<li><p>The job id issued was 4963</p></li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="parallel-jobs">
<h2><span class="section-number">8.3. </span>Parallel Jobs<a class="headerlink" href="#parallel-jobs" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><dl class="simple">
<dt>Here, we will only discuss parallel jobs</dt><dd><ul>
<li><p>by running several instances of a single-threaded program (so-called embarrassingly parallel paradigm or a job array)</p></li>
<li><p>by running a multithreaded program (shared memory paradigm, e.g. with OpenMP or pthreads)</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Other types of parallel jobs: see <a class="reference external" href="https://support.ceci-hpc.be/doc/_contents/QuickStart/SubmittingJobs/SlurmTutorial.html#going-parallel/">Ceci - see Going parallel</a>.</p></li>
<li><dl class="simple">
<dt>From this same website: Tasks are requested/created with the <cite>–ntasks</cite> option, while CPUs, for the multithreaded programs, are requested with the <cite>–cpus-per-task</cite> option. Tasks can be split across several compute nodes, so requesting several CPUs with the <cite>–cpus-per-task</cite> option will ensure all CPUs are allocated on the same compute node. By contrast, requesting the same amount of CPUs with the <cite>–ntasks</cite> option may lead to several CPUs being allocated on several, distinct compute nodes.</dt><dd><ul>
<li><p>Multithreaded programs run on one specific compute node: use the <cite>–cpus-per-task</cite> flag with these programs.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="section" id="multithreaded-bowtie2-example">
<h3><span class="section-number">8.3.1. </span>Multithreaded bowtie2 Example<a class="headerlink" href="#multithreaded-bowtie2-example" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><dl class="simple">
<dt>Many genomics software use a multithreaded approach. We start with a bowtie2 example:</dt><dd><ul>
<li><p>We want to align 2 fastq files from the European Nucleotide Archive to the Mycoplasma G37 genome.</p></li>
<li><dl class="simple">
<dt>Workflow:</dt><dd><ul>
<li><p>download the G37 genome to the /scratch directory of the node</p></li>
<li><p>build the genome index on this /scratch directory</p></li>
<li><p>download the fastq files to scratch</p></li>
<li><p>do the alignment</p></li>
<li><p>store the resulting sam files in your <em>personal</em> directory.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Our batch script is below (save it as align_Mycoplasma and run it with <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">align_Mycoplasma</span></code> and monitor it with <code class="docutils literal notranslate"><span class="pre">squeue</span></code>):</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --job-name=align_Mycoplasma</span>
<span class="c1">#SBATCH --output=res_alignjob.txt</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=8</span>
<span class="c1">#SBATCH --time=10:00</span>
<span class="c1">#SBATCH --mem-per-cpu=2000</span>
<span class="c1">#</span>
<span class="nb">cd</span> /scratch
srun wget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/027/325/GCF_000027325.1_ASM2732v1/GCF_000027325.1_ASM2732v1_genomic.fna.gz -P ./
srun bowtie2-build GCF_000027325.1_ASM2732v1_genomic.fna.gz MG37
srun wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486827/ERR486827_1.fastq.gz -P ./
srun wget ftp://ftp.sra.ebi.ac.uk/vol1/fastq/ERR486/ERR486827/ERR486827_2.fastq.gz -P ./
srun bowtie2 -x MG37 -1 ERR486827_1.fastq.gz -2 ERR486827_2.fastq.gz --very-fast -p <span class="nv">$SLURM_CPUS_PER_TASK</span> -S /zfs/omics/personal/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/result.sam
</pre></div>
</div>
<ul class="simple">
<li><p>the output of the job (res_alignjob.txt) is stored where you execute the sbatch. It contains the information that is normally written to your standard output (your screen). In this case, the progress of the download, the progress of the indexing and the alignment summary.</p></li>
<li><p>the actual result of the alignment (the sam file) is written to your <em>personal</em> directory.</p></li>
<li><p>the number of threads in the bowtie2 command (job step) is taken from the <em>SLURM variable</em> <code class="docutils literal notranslate"><span class="pre">$SLURM_CPUS_PER_TASK</span></code> that was given at the start of the job. You could have given any number up to 8 with the -p flag. When you issue a number &gt;8 the job will still be executed with the number of threads defined by <code class="docutils literal notranslate"><span class="pre">$SLURM_CPUS_PER_TASK</span></code> (in this case 8).</p></li>
</ul>
</div>
</div>
<div class="section" id="multithreaded-example-in-c">
<h2><span class="section-number">8.4. </span>Multithreaded Example in C<a class="headerlink" href="#multithreaded-example-in-c" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>The example below is a C code illustration of how treads are forked from a master thread. This idea can be used when you make your own parallelized code.</p></li>
<li><p>Save the file below as omp_hoi.c:</p></li>
</ul>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="cm">/******************************************************************************</span>
<span class="cm">* FILE: omp_hoi.c</span>
<span class="cm">* DESCRIPTION:</span>
<span class="cm">*   OpenMP Example - Hello World - C/C++ Version</span>
<span class="cm">*   In this simple example, the master thread forks a parallel region.</span>
<span class="cm">*   All threads in the team obtain their unique thread number and print it.</span>
<span class="cm">*   The master thread only prints the total number of threads.  Two OpenMP</span>
<span class="cm">*   library routines are used to obtain the number of threads and each</span>
<span class="cm">*   thread&#39;s number.</span>
<span class="cm">* AUTHOR: Blaise Barney  5/99</span>
<span class="cm">* LAST REVISED: 04/06/05</span>
<span class="cm">******************************************************************************/</span>
<span class="cp">#include</span> <span class="cpf">&lt;omp.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdio.h&gt;</span><span class="cp"></span>
<span class="cp">#include</span> <span class="cpf">&lt;stdlib.h&gt;</span><span class="cp"></span>
<span class="cp">#</span>
<span class="kt">int</span> <span class="nf">main</span> <span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="kt">int</span> <span class="n">nthreads</span><span class="p">,</span> <span class="n">tid</span><span class="p">;</span>
<span class="cm">/* Fork a team of threads giving them their own copies of variables */</span>
<span class="cp">#pragma omp parallel private(nthreads, tid)</span>
<span class="p">{</span>
        <span class="cm">/* Obtain thread number */</span>
        <span class="n">tid</span> <span class="o">=</span> <span class="n">omp_get_thread_num</span><span class="p">();</span>
        <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello World from Crunchomics thread = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">tid</span><span class="p">);</span>
        <span class="cm">/* Only master thread does this */</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">tid</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">{</span>
                <span class="n">nthreads</span> <span class="o">=</span> <span class="n">omp_get_num_threads</span><span class="p">();</span>
                <span class="n">printf</span><span class="p">(</span><span class="s">&quot;Master thread says: number of threads = %d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">nthreads</span><span class="p">);</span>
        <span class="p">}</span>
  <span class="p">}</span>  <span class="cm">/* All threads join master thread and disband */</span>
<span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>and compile it:</p></li>
</ul>
<div class="highlight-C notranslate"><div class="highlight"><pre><span></span><span class="n">gcc</span> <span class="o">-</span><span class="n">fopenmp</span> <span class="n">omp_hoi</span><span class="p">.</span><span class="n">c</span> <span class="o">-</span><span class="n">o</span> <span class="n">hoi</span><span class="p">.</span><span class="n">omp</span>
</pre></div>
</div>
<ul class="simple">
<li><p>run it through the following sbatch script:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#SBATCH --job-name=test_omp</span>
<span class="c1">#SBATCH --output=res_omp.txt</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=5</span>
<span class="nb">export</span> <span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="nv">$SLURM_CPUS_PER_TASK</span>
./hoi.omp
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/Slurm4.png"><img alt="_images/Slurm4.png" src="_images/Slurm4.png" style="width: 400px; height: 160px;" /></a>
</div>
<div class="section" id="job-arrays-parallel-example-with-rscript">
<h3><span class="section-number">8.4.1. </span>Job Arrays: Parallel example with Rscript<a class="headerlink" href="#job-arrays-parallel-example-with-rscript" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>You can use the sbatch <code class="docutils literal notranslate"><span class="pre">--array=&lt;indexes&gt;</span></code> parameter to submit a job array, i.e., multiple jobs to be executed with identical parameters. The indexes specification identifies what array index values should be used.</p></li>
<li><p>Suppose we study 9 chromosomes of a certain organism (for which we have 9 files, 9 R-objects or something of the kind that we want to process).</p></li>
<li><p>Below a sbatch script is shown that spawns 9 sub-tasks (the array starting with 0 to 8). With each sub-task an R script is run that uses the element of the vector <code class="docutils literal notranslate"><span class="pre">${CHROMS}</span></code> as indicated by the index of the array. Thus, 9 Rscript-processes are run, each for a chromosome:</p></li>
<li><p>copy the code below in a file called parJobBatch.sh</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --job-name=ParTest</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --array=0-8</span>
<span class="c1">#</span>
<span class="nv">CHROMS</span><span class="o">=(</span><span class="s2">&quot;chr1&quot;</span> <span class="s2">&quot;chr2&quot;</span> <span class="s2">&quot;chr3&quot;</span> <span class="s2">&quot;chr4&quot;</span> <span class="s2">&quot;chr5&quot;</span> <span class="s2">&quot;chr6&quot;</span> <span class="s2">&quot;chrX&quot;</span> <span class="s2">&quot;chrY&quot;</span> <span class="s2">&quot;Mit&quot;</span><span class="o">)</span>
<span class="c1">#</span>
srun Rscript parJob.R <span class="si">${</span><span class="nv">CHROMS</span><span class="p">[</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="p">]</span><span class="si">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p>copy the code below in a file called parJob.R</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">args</span><span class="o">=</span>commandArgs<span class="o">(</span><span class="nv">trailingOnly</span><span class="o">=</span>TRUE<span class="o">)</span>
<span class="nv">chromosome</span><span class="o">=</span>args<span class="o">[</span><span class="m">1</span><span class="o">]</span>
<span class="c1">#</span>
cat<span class="o">(</span><span class="s2">&quot;Start work on&quot;</span>,chromosome,<span class="s2">&quot;\n&quot;</span><span class="o">)</span>
cat<span class="o">(</span><span class="s2">&quot;working ....\n&quot;</span><span class="o">)</span>
<span class="c1"># put the code what you want to do with each chromosome here</span>
Sys.sleep<span class="o">(</span><span class="m">20</span><span class="o">)</span>
cat<span class="o">(</span><span class="s2">&quot;Done work on&quot;</span>,chromosome,<span class="s2">&quot;\n&quot;</span><span class="o">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The job is started with <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">parJobBatch.sh</span></code></p></li>
<li><p>The job 9 job steps each have an  entry in the queue</p></li>
</ul>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/slurm6.png"><img alt="_images/slurm6.png" src="_images/slurm6.png" style="width: 615px; height: 140px;" /></a>
</div>
<ul class="simple">
<li><dl class="simple">
<dt>Result:</dt><dd><ul>
<li><p>9 files each outputted by a different R process. With content such as:</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat slurm_<span class="o">[</span>jobid<span class="o">]</span>_0.txt
</pre></div>
</div>
<p>Where the number after the underscore is the jobstep number:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Start</span> <span class="n">work</span> <span class="n">on</span> <span class="n">chr1</span>
<span class="n">working</span> <span class="o">....</span>
<span class="n">Done</span> <span class="n">work</span> <span class="n">on</span> <span class="n">chr1</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Rscript was used here to illustrate the <code class="docutils literal notranslate"><span class="pre">--array</span></code> functionality, any program which has to be run for a number of inputs  can be set up in this way.</p></li>
<li><p>A list of files can be used instead of chromosomes. If there are 20 file to be processed the relevant part of the batch script would look something like:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">#SBATCH --array=0-19</span>
<span class="nv">FILES</span><span class="o">=(</span>/zfs/omics/personal/*<span class="o">)</span>
srun program <span class="si">${</span><span class="nv">FILES</span><span class="p">[</span><span class="nv">$SLURM_ARRAY_TASK_ID</span><span class="p">]</span><span class="si">}</span>
</pre></div>
</div>
</div>
<div class="section" id="use-conda-environments-on-the-compute-nodes">
<h3><span class="section-number">8.4.2. </span>Use Conda Environments on the Compute Nodes<a class="headerlink" href="#use-conda-environments-on-the-compute-nodes" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Run the flye assembly (see 6.2.1) on a compute node using sbatch.</p></li>
<li><p>Remark: before you execute the sbatch command, activate the proper conda environmnent. In this case it is necessary to activate nptools because flye was installed in this environment. <code class="docutils literal notranslate"><span class="pre">activate</span> <span class="pre">conda</span> <span class="pre">nptools</span></code> The activation will be passed to the compute nodes.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/bin/bash</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --job-name=ecoli_assemble</span>
<span class="c1">#SBATCH --output=res_ecoli_assembly.txt</span>
<span class="c1">#</span>
<span class="c1">#SBATCH --ntasks=1</span>
<span class="c1">#SBATCH --cpus-per-task=8</span>
<span class="c1">#SBATCH --time=20:00</span>
<span class="c1">#SBATCH --mem-per-cpu=32000</span>
<span class="c1">#</span>
<span class="nv">SCRATCH</span><span class="o">=</span>/scratch/<span class="nv">$USER</span>/ecoli
mkdir -m <span class="m">700</span> -p <span class="nv">$SCRATCH</span>
<span class="nb">cd</span> <span class="nv">$SCRATCH</span>
srun wget https://zenodo.org/record/1172816/files/Loman_E.coli_MAP006-1_2D_50x.fasta
date
srun flye --nano-raw Loman_E.coli_MAP006-1_2D_50x.fasta --out-dir /zfs/omics/personal/<span class="si">${</span><span class="nv">USER</span><span class="si">}</span>/ecoli-batch --threads     <span class="nv">$SLURM_CPUS_PER_TASK</span>
date
</pre></div>
</div>
<ul class="simple">
<li><p>Save this file as assemble_ecoli.sh and run from the headnode <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">assemble_ecoli.sh</span></code></p></li>
</ul>
</div>
</div>
<div class="section" id="interactive-shells-continued">
<h2><span class="section-number">8.5. </span>Interactive Shells Continued<a class="headerlink" href="#interactive-shells-continued" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>For interactive work: use the head node.</p></li>
<li><dl class="simple">
<dt>In some cases it might be convenient to have shell access on the compute node, for instance to look at the memory and cpu allocation of a specific process.</dt><dd><ul>
<li><p><strong>limit the duration of this shell by issueing the -t &lt;min&gt;</strong></p></li>
<li><p>use the -w flag to go to the node you want your shell to live in.</p></li>
<li><p>have a shell on cn001 for 1 minute:</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hostname
<span class="c1">#omics-h0.science.uva.nl</span>
srun -n <span class="m">1</span>  -t <span class="m">1</span> --pty -w omics-cn001 bash -i
hostname
<span class="c1">#omics-cn001</span>
</pre></div>
</div>
<div class="section" id="example-using-an-interactive-shell">
<h3><span class="section-number">8.5.1. </span>Example using an interactive shell<a class="headerlink" href="#example-using-an-interactive-shell" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>Re-run the <em>Multithreaded bowtie2 example</em>. Configure to use 2 threads. Use squeue to find out the node on which the job runs. Then, from the head node (a shell for a minute on -in this case- cn001):</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sbatch
srun -n <span class="m">1</span>  -t <span class="m">1</span> --pty -w omics-cn001 bash -i
</pre></div>
</div>
<div class="figure align-center">
<a class="reference internal image-reference" href="_images/slurm7.png"><img alt="_images/slurm7.png" src="_images/slurm7.png" style="width: 575px; height: 233px;" /></a>
</div>
<ul class="simple">
<li><p>I see with <code class="docutils literal notranslate"><span class="pre">squeue</span></code> that my alignment script is running as slurm job 7094 on compute node cn001. Hence, I start an interactive shell at compute node 001 (for a minute) and monitor with <code class="docutils literal notranslate"><span class="pre">top</span></code> that indeed there is a processor load of 200% (2 threads) used by the bowtie2-align-s program.</p></li>
</ul>
</div>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">8. Slurm Jobs</a><ul>
<li><a class="reference internal" href="#jobs-and-job-steps">8.1. Jobs and Job Steps</a></li>
<li><a class="reference internal" href="#batch-jobs-sbatch">8.2. Batch jobs: sbatch</a></li>
<li><a class="reference internal" href="#parallel-jobs">8.3. Parallel Jobs</a><ul>
<li><a class="reference internal" href="#multithreaded-bowtie2-example">8.3.1. Multithreaded bowtie2 Example</a></li>
</ul>
</li>
<li><a class="reference internal" href="#multithreaded-example-in-c">8.4. Multithreaded Example in C</a><ul>
<li><a class="reference internal" href="#job-arrays-parallel-example-with-rscript">8.4.1. Job Arrays: Parallel example with Rscript</a></li>
<li><a class="reference internal" href="#use-conda-environments-on-the-compute-nodes">8.4.2. Use Conda Environments on the Compute Nodes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#interactive-shells-continued">8.5. Interactive Shells Continued</a><ul>
<li><a class="reference internal" href="#example-using-an-interactive-shell">8.5.1. Example using an interactive shell</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="slurm_overview.html"
                        title="previous chapter"><span class="section-number">7. </span>Slurm Overview</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="slurm_indirect.html"
                        title="next chapter"><span class="section-number">9. </span>Using Slurm Indirectly</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/slurm_jobs.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="slurm_indirect.html" title="9. Using Slurm Indirectly"
             >next</a> |</li>
        <li class="right" >
          <a href="slurm_overview.html" title="7. Slurm Overview"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">Crunchomics 0.01 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href=""><span class="section-number">8. </span>Slurm Jobs</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2021, Han Rauwerda Wim de Leeuw SILS-MAD-RB&amp;AB.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 3.4.3.
    </div>
  </body>
</html>